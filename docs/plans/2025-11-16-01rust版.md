オーバーウォッチ2をOBSで配信している際に、リアルタイムに勝敗判定を行い画面表示に反映するソフトウェアを実装します。

PoCとして、Pythonで実装を行い試してみた結果、概ねやりたいことが実現できることがわかりました。
このPoCは、 `packages/obs-victory-counter` ディレクトリーにあります。

今回は、必要な機能を整理し、rustで実装しなおしていきます。
rustで実装する理由は以下になります:

- PythonやNodeで実行するプログラムだと、エンドユーザーが実行環境をセットアップする必要が出てしまう
- ゲームのバックグランドで動作させるので、なるべく軽量であるのが望ましい

今回実装するソフトウェア構成は次のようなものを考えています:

- OBSプラグイン
  - オーバーウォッチ2プレイ画面を取得するプラグイン
  - Rustで実装
  - PoCではWebSocket経由で取得していたが、その方式より軽量、低レイテンシーでの処理が期待できる
- 実行ファイル
  - Rustで実装
  - Webサーバー兼画像判定機能を持つ
  - Webサーバー機能
    - 勝敗カウントを管理し、勝敗数を応答する
    - ブラウザーへリアルタイムに勝敗数を通知する
      - WebSocketが向いている？
    - フロントはCDN版Vue。理由は後述
  - 画像判定機能
    - 前述OBSプラグインを経由してゲーム画面を取得する
    - 事前に学習したパターンと比較し勝利化敗北を判定する
      - ロジックはPoCで検証したものと同じ
- 学習プログラム
  - PoCで実装したものと同じ。ただしrustで実装する

エンドユーザーに配布するファイルは次のようなものを考えています:

- OBSプラグイン
- Rust実行ファイル
- 画面レイアウト用設定ファイル
  - CSSやコンフィグ
  - エンドユーザーのスキルによっては、フロントエンドファイル直接編集でレイアウト編集も可能にする
    - なので、CDN版Vueが適しているのではと考えた

検討したいこと:

- 前述の「OBSプラグイン」と「実行ファイル」は1バイナリーにまとめることは可能？可能だとしてそうすべき？

---

## PoCコードの分析結果

既存のPython PoCコードを調査した結果、以下の構造が明らかになりました：

### アーキテクチャ

- **コード量**: 約1,300行（コア実装のみ）
- **主要コンポーネント**:
  - `StateManager` (337行): イベントログ管理、2段階クールダウン、連続検知
  - `VictoryPredictor` (179行): CNN推論、画像前処理パイプライン
  - HTTP APIサーバー (505行): `/state`, `/history`, `/adjust`, `/overlay`エンドポイント

### 重要な機能

1. **2段階クールダウン**:

   - READY → (勝敗検知) → COOLDOWN (180秒) → WAITING_FOR_NONE (none 50回連続) → READY
   - 重複カウント防止と放置時の誤検知防止

2. **連続検知**:

   - デフォルト2回連続で同じ結果を検知したらカウント確定
   - 0.25秒間隔 × 2回 = 0.5秒で判定完了

3. **イベントソーシング**:

   - すべてのイベントをJSONL形式で永続化
   - プロセス再起動後も状態復元可能

4. **OBS連携**:

   - obs-websocket経由でスクリーンショット取得
   - Base64エンコード/デコード

5. **CNN推論**:
   - クロップ領域: `(460, 378, 995, 550)`
   - マスク機能: `(0, 534, 1920, 295)` - 画面下部を黒塗り
   - 5クラス分類: victory_text, victory_progressbar, defeat_text, defeat_progressbar, none

### 依存ライブラリ

| ライブラリ    | 用途          | Rust代替候補              |
| ------------- | ------------- | ------------------------- |
| numpy         | 配列操作      | ndarray                   |
| opencv-python | 画像処理      | opencv-rust               |
| torch         | CNN推論       | tch-rs または onnxruntime |
| obsws-python  | OBS WebSocket | obws                      |
| http.server   | HTTP API      | axum または actix-web     |

---

## 設計上の選択肢の比較

### 1. OBSプラグイン vs 実行ファイルの構成

#### オプションA: 1つのバイナリにまとめる（OBSプラグイン）

**実現可能性**: ✅ 可能（Rustで C FFI を使用）

**メリット**:

- ✅ 配布ファイルが1つで済む
- ✅ ユーザーのセットアップが簡単
- ✅ OBS内部メモリから直接画像取得可能（最速）

**デメリット**:

- ❌ OBSプロセスのクラッシュリスク
- ❌ OBSのプラグインAPIの制約（Tokioランタイムとの相性問題の可能性）
- ❌ デバッグが困難
- ❌ プラグイン開発の複雑さ

#### オプションB: 2つのバイナリに分離 ⭐推奨

**実現可能性**: ✅ 可能（PoCと同様のアーキテクチャ）

**メリット**:

- ✅ 各コンポーネントが独立して動作（安定性向上）
- ✅ 実行ファイル側は通常のRustアプリ開発（デバッグ容易）
- ✅ OBSクラッシュのリスクゼロ
- ✅ 段階的な開発・テストが可能

**デメリット**:

- ❌ obs-websocket経由でのオーバーヘッド（Base64エンコード）
- ❌ 配布ファイルが2つ
- ❌ ユーザーが2つのプロセスを管理する必要

**推奨理由**: 安定性とデバッグ容易さを優先。obs-websocketのオーバーヘッドは0.25秒間隔なら許容範囲。

#### オプションC: OBSプラグインなし（obs-websocket継続）

**実現可能性**: ✅ 可能（PoCそのまま）

**メリット**:

- ✅ 最もシンプル（プラグイン開発不要）
- ✅ PoCで実証済み

**デメリット**:

- ❌ ドキュメントの要件「OBSプラグイン」を満たさない
- ❌ レイテンシー改善が限定的

---

### 2. CNNモデルのRust移植方法

#### オプションA: ONNX形式に変換 ⭐推奨

**メリット**:

- ✅ クロスプラットフォーム性が高い
- ✅ 依存ライブラリが軽量（`onnxruntime-rs`のみ）
- ✅ CPU/GPU両対応
- ✅ モデル最適化の恩恵

**デメリット**:

- ❌ PyTorchからONNXへの変換作業が必要（`torch.onnx.export`）
- ❌ 変換時にサポートされないオペレーションがある可能性

**推奨理由**: 配布の簡便さとクロスプラットフォーム性。変換作業は一度きり。

#### オプションB: tch-rs（PyTorchバインディング）

**メリット**:

- ✅ 既存の`.pth`ファイルをそのまま使用可能
- ✅ 変換作業不要

**デメリット**:

- ❌ libtorchのインストールが必要（配布サイズ増大: 数百MB）
- ❌ バイナリ配布が複雑
- ❌ クロスコンパイルが困難

#### オプションC: Rustネイティブ（burn/candle）

**メリット**:

- ✅ 完全なRust実装（依存が最小）
- ✅ バイナリサイズが最小

**デメリット**:

- ❌ 学習済み重みの移行作業が必要
- ❌ モデルをRustで再実装する必要
- ❌ 学習プログラムもRustで書き直す必要

---

### 3. ブラウザへのリアルタイム通知方法

#### オプションA: WebSocket

**メリット**:

- ✅ 真の双方向通信
- ✅ 最低レイテンシー

**デメリット**:

- ❌ 実装がやや複雑（接続管理、再接続ロジック）
- ❌ PoCより複雑化

#### オプションB: Server-Sent Events (SSE) ⭐推奨

**メリット**:

- ✅ サーバー→ブラウザのプッシュ通知が可能
- ✅ HTTP/2との相性が良い
- ✅ 自動再接続機能（ブラウザ標準）
- ✅ WebSocketより実装が簡単

**デメリット**:

- ❌ 一方向通信のみ（サーバー→ブラウザ）

**補正リクエストについて**: ✅ 可能

- SSEはサーバー→ブラウザの通知専用
- ブラウザ→サーバーの補正リクエストは通常のHTTP POST `/adjust`で実装
- PoCと同じAPI設計が可能

**推奨理由**: リアルタイム性とシンプルさのバランス。補正リクエストは通常のPOSTで実現可能。

#### オプションC: ポーリング（PoCと同様）

**メリット**:

- ✅ 実装が最もシンプル
- ✅ PoCで実証済み

**デメリット**:

- ❌ 5秒間隔の遅延
- ❌ リアルタイム性が低い

---

### 4. フロントエンド配置方法

#### 推奨アプローチ: ハイブリッド方式

**HTML/JS（Vue CDN）**: バイナリに組み込み

- `include_str!("../frontend/index.html")` で組み込み
- ユーザーが誤って壊すことがない

**CSS**: 別ファイルとして配布

- `frontend/style.css` として配布
- ユーザーがカスタマイズ可能

**ディレクトリ構成**:

```
victory-detector/
├── victory-detector.exe (または .so)
└── frontend/
    ├── style.css      # ユーザーがカスタマイズ可能
    └── config.json    # 表示設定（オプション）
```

**実装**:

- HTMLは`/overlay`エンドポイントで組み込み版を返す
- CSSは`GET /frontend/style.css`で外部ファイルを配信
- ファイルが存在しない場合はデフォルトCSSをバイナリから返す

---

## 推奨アプローチのまとめ

1. **バイナリ構成**: 2つのバイナリに分離（安定性重視）
2. **モデル形式**: ONNX形式（配布の簡便さ）
3. **通知方式**: SSE + HTTP POST（リアルタイム性とシンプルさのバランス）
4. **フロントエンド**: ハイブリッド方式（HTMLは組み込み、CSSは別ファイル）

---

## 実装プラン概要

### ディレクトリ構成

```
packages/
├── obs-plugin/           # OBSプラグイン（軽量、オプション）
│   ├── Cargo.toml
│   └── src/
│       └── lib.rs       # obs-websocketへのブリッジ
└── victory-detector-rs/ # メイン実行ファイル
    ├── Cargo.toml
    ├── src/
    │   ├── main.rs
    │   ├── core/
    │   │   ├── state.rs      # StateManager
    │   │   ├── vision.rs     # DetectionResult
    │   │   └── cooldown.rs   # CooldownState
    │   ├── inference/
    │   │   └── predictor.rs  # ONNX推論
    │   ├── server/
    │   │   ├── mod.rs
    │   │   ├── routes.rs
    │   │   └── sse.rs
    │   └── cli.rs
    ├── frontend/
    │   ├── index.html
    │   └── style.css
    └── models/
        └── victory_classifier.onnx
```

### 主要クレート

```toml
[dependencies]
# 非同期ランタイム
tokio = { version = "1", features = ["full"] }

# Webサーバー
axum = "0.7"
tower-http = { version = "0.5", features = ["cors", "fs"] }

# ONNX推論
ort = "2"  # onnxruntime

# 画像処理
image = "0.25"
opencv = { version = "0.92", features = ["imgproc"] }

# OBS連携
obws = "0.11"

# JSON/シリアライゼーション
serde = { version = "1", features = ["derive"] }
serde_json = "1"

# ログ
tracing = "0.1"
tracing-subscriber = "0.3"

# エラーハンドリング
anyhow = "1"
thiserror = "1"

# CLI
clap = { version = "4", features = ["derive"] }

# 時刻
chrono = { version = "0.4", features = ["serde"] }
```

### 実装フェーズ

1. **Phase 1**: プロジェクト構造とコア機能（StateManager、イベントログ）
2. **Phase 2**: ONNX推論とOBS連携
3. **Phase 3**: HTTP API + SSE
4. **Phase 4**: フロントエンド（Vue.js CDN版）
5. **Phase 5**: 学習プログラム（Pythonスクリプト維持 + ONNX変換）
6. **Phase 6**: OBSプラグイン（オプション）
7. **Phase 7**: テスト・ドキュメント

### API設計

```
# SSE（リアルタイム通知）
GET /events -> Stream<Event>

# HTTP API（PoCと互換）
GET /state -> CounterState
GET /history?limit=N -> Vec<Event>
POST /adjust -> CounterState

# フロントエンド
GET /overlay -> HTML
GET /frontend/style.css -> CSS
```

### 配布物

**Windows**:

```
victory-detector-v1.0-windows.zip
├── victory-detector.exe
├── models/
│   └── victory_classifier.onnx
├── frontend/
│   └── style.css
├── README.txt
└── config.toml （オプション）
```

---

## OBSプラグイン vs obs-websocket の詳細分析

### obs-websocketの画像転送メカニズム

現在のPoCでは以下の処理フローになっています：

```
OBS（C++）
  → スクリーンショット取得
  → PNG圧縮
  → Base64エンコード
  → WebSocket送信
      ↓
Python/Rustプロセス
  → WebSocket受信
  → Base64デコード
  → PNGデコード（cv2/image crate）
  → numpy配列/Tensor変換
  → CNN推論
```

**obs-websocketの仕様**:

- `get_source_screenshot(source, "png", width, height, quality)` APIを使用
- 戻り値: `{"imageData": "data:image/png;base64,iVBORw0KG..."}`
- JSONベースのため、画像はBase64文字列として返される
- これはobs-websocket APIの設計であり、変更不可

**Base64オーバーヘッド**:

| 項目                  | 値                     |
| --------------------- | ---------------------- |
| データサイズ増加      | +33%（3バイト→4文字）  |
| 1920×1080 PNG サイズ  | 約 500KB（圧縮後）     |
| Base64 エンコード後   | 約 665KB               |
| 0.25秒間隔での転送量  | 2.66MB/秒              |
| localhost 転送速度    | 問題なし（十分高速）   |
| Base64 エンコード時間 | 約 1-2ms（CPU依存）    |
| Base64 デコード時間   | 約 1-2ms（CPU依存）    |
| PNG デコード時間      | 約 10-20ms（最大負荷） |

**結論**: Base64のオーバーヘッドは軽微。**最大のボトルネックはPNGデコード**。

### OBSプラグインを作った場合のメリット

#### オプション1: 共有メモリ方式

```
OBS（C++）+ プラグイン
  → フレームバッファから直接取得（非圧縮RGB）
  → 共有メモリへ書き込み
      ↓
Rustプロセス
  → 共有メモリから読み取り
  → Tensor変換（圧縮・デコード不要）
  → CNN推論
```

**メリット**:

- PNG圧縮/デコード不要 → **10-20ms削減**
- Base64エンコード/デコード不要 → **2-4ms削減**
- 合計: **約12-24ms削減**（1フレームあたり）

**デメリット**:

- 共有メモリ実装の複雑さ
- プラットフォーム依存（Windows/Linux で異なる）
- OBSプラグインのC++ FFI実装

#### オプション2: gRPC/ProtoBufバイナリ転送

```
OBS（C++）+ プラグイン
  → フレームバッファから取得
  → ProtoBuf（バイナリ）でシリアライズ
  → gRPC送信
      ↓
Rustプロセス
  → gRPC受信
  → ProtoBufデシリアライズ
  → Tensor変換
  → CNN推論
```

**メリット**:

- Base64不要（バイナリ転送）
- 型安全なAPI
- クロスプラットフォーム

**デメリット**:

- gRPC実装の複雑さ
- PNG圧縮は依然として必要（またはJPEG等）
- OBSプラグイン開発の学習コスト

### パフォーマンス比較

| 方式                | エンコード | デコード | 転送    | 合計（推定） | 実装難易度 |
| ------------------- | ---------- | -------- | ------- | ------------ | ---------- |
| obs-websocket(現状) | 3-4ms      | 1-2ms    | 10-20ms | **14-26ms**  | ★☆☆☆☆      |
| 共有メモリ          | 0ms        | 0ms      | 0-1ms   | **0-1ms**    | ★★★★★      |
| gRPC バイナリ       | 1-2ms      | 1-2ms    | 5-10ms  | **7-14ms**   | ★★★★☆      |

**0.25秒間隔（250ms）での影響**:

- obs-websocket: 14-26ms / 250ms = **5-10%のオーバーヘッド**
- 共有メモリ: 0-1ms / 250ms = **0-0.4%のオーバーヘッド**

### 推奨判断

#### obs-websocket継続を推奨する理由

1. **実用上のパフォーマンスは十分**

   - 0.25秒間隔で14-26msのオーバーヘッドは許容範囲
   - PoCで実証済み

2. **実装の簡潔さ**

   - obs-websocketはOBS 28以降で標準搭載
   - Rustの`obws`クレートは成熟している
   - 開発時間を大幅に削減

3. **安定性**

   - OBSプロセスから独立して動作
   - クラッシュリスクなし
   - デバッグが容易

4. **メンテナンス性**
   - obs-websocketのアップデートに追随しやすい
   - プラットフォーム固有のコードが不要

#### OBSプラグインを作るべきケース

以下の条件を**すべて**満たす場合のみ検討：

1. **レイテンシーが致命的** - 現状の14-26msが許容できない（配信には影響なし）
2. **開発リソースが十分** - C++ FFI、共有メモリ、OBS APIの学習に数週間〜1ヶ月
3. **Windows以外もターゲット** - Linux/macOSでも配布する予定

### 結論

**推奨: obs-websocket継続（オプションC）**

理由:

- Base64のオーバーヘッドは理論上33%のサイズ増だが、実際のCPUコストは軽微（1-2ms）
- 最大のボトルネックはPNGデコード（10-20ms）だが、0.25秒間隔なら十分高速
- 5-10%のオーバーヘッドは配信品質に影響なし
- 実装の複雑さとメンテナンスコストを大幅に削減

**ただし**: 将来的にレイテンシー改善が必要になった場合、Phase 6でOBSプラグインを追加実装する選択肢を残す。

---

## アーキテクチャオプションの再検討

### オプションD: OBSプラグイン内で完結（All-in-One）

すべての機能をOBSプラグイン（.so/.dll）内に統合する案：

```
OBS プロセス内
├── OBSプラグイン（Rust、.so/.dll）
    ├── 画像取得（OBS内部API）
    ├── CNN推論（ONNX Runtime）
    ├── StateManager
    ├── HTTPサーバー（axum + tokio）
    └── SSE配信
```

#### メリット

1. **最速のパフォーマンス**

   - 画像転送不要（OBS内部メモリから直接取得）
   - Base64エンコード/デコード不要
   - PNGエンコード/デコード不要
   - 推定オーバーヘッド: **0-2ms**（Tensor変換のみ）

2. **配布の簡潔さ**

   - ファイル1つ（OBSプラグイン + 組み込みモデル）
   - ユーザーは「OBSプラグインフォルダに入れる」だけ
   - 実行ファイルの起動不要（OBS起動で自動開始）

3. **ユーザー体験の向上**
   - プロセス管理不要（OBSのみ起動）
   - 設定が簡単

#### デメリット

1. **OBSへの影響（致命的な可能性）**

   | 項目                   | 影響                                      |
   | ---------------------- | ----------------------------------------- |
   | CPU使用率              | CNN推論で+10-30%（コア1つ占有）           |
   | GPU使用率              | CUDA推論時、OBSエンコーディングと競合     |
   | メモリ使用量           | +500MB-1GB（ONNXランタイム+モデル+依存）  |
   | OBSの応答性            | 推論中にOBS UIがフリーズする可能性        |
   | 配信エンコード品質低下 | GPU/CPU リソース競合で配信fpsが低下の恐れ |

2. **安定性リスク**

   - プラグインのクラッシュ → **OBS全体がクラッシュ**
   - 配信中のクラッシュは致命的
   - Rustのpanic → OBSプロセス終了

3. **技術的課題**

   - **Tokio非同期ランタイム**をOBSプロセス内で起動
     - OBSのイベントループとの競合
     - スレッド管理の複雑さ
   - **HTTPサーバー**をプラグイン内で実行
     - OBSのネットワーク設定との競合
     - ポート管理の問題
   - **ONNX Runtimeの初期化**
     - OBSプラグインロード時に重い初期化処理
     - OBS起動時間が数秒増加

4. **デバッグの困難さ**

   - OBSプロセス内でしか動作しない
   - ログ出力がOBSログに埋もれる
   - デバッガのアタッチが困難
   - 推論精度の検証が難しい

5. **開発・テストの複雑さ**

   - OBS起動なしでテスト不可
   - 単体テストが困難（OBS環境が必要）
   - CI/CDでの自動テストが難しい
   - 開発イテレーションが遅い（OBS再起動が必要）

6. **GPU競合の深刻度**

   **シナリオ**: RTX 3070で配信中

   - OBSエンコーディング: GPU使用率 30-50%（NVENC）
   - CNN推論（CUDA）: GPU使用率 +20-40%
   - **合計**: 50-90% → **エンコード品質低下またはfps低下**

   **ワークアラウンド**: CPU推論を強制

   - → CPU使用率が急増（+30-50%）
   - → ゲームfpsに影響

#### オプションD vs オプションB（2バイナリ分離）比較

| 項目                 | オプションD（All-in-One） | オプションB（分離）    |
| -------------------- | ------------------------- | ---------------------- |
| 画像転送オーバー     | 0ms                       | 14-26ms                |
| OBSへの影響          | ❌ 大（リソース競合）     | ✅ なし                |
| クラッシュ時の影響   | ❌ 配信停止               | ✅ 検知機能のみ停止    |
| デバッグ容易性       | ❌ 困難                   | ✅ 容易                |
| テスト容易性         | ❌ OBS必須                | ✅ 独立して実行可能    |
| GPU競合              | ❌ あり                   | ✅ なし（別プロセス）  |
| 配布ファイル数       | ✅ 1つ                    | ❌ 2つ                 |
| ユーザーセットアップ | ✅ 簡単                   | ❌ やや複雑            |
| 開発時間             | ❌ 4-6週間                | ✅ 2-3週間             |
| 実装難易度           | ❌ ★★★★★                  | ✅ ★★☆☆☆               |
| パフォーマンス改善   | 14-26ms削減               | 基準                   |
| 配信への影響リスク   | ❌ 高い                   | ✅ なし                |
| 将来の拡張性         | ❌ 低い（OBS依存）        | ✅ 高い（独立したAPI） |

#### 評価

**オプションDは推奨しない**

##### 致命的な問題

1. **配信品質への影響リスク**

   - GPU/CPU競合で配信fpsが低下する可能性
   - 配信中のクラッシュリスク
   - これらは本末転倒（配信補助ツールが配信を阻害）

2. **開発・保守コストが高すぎる**

   - 14-26msの改善のために、4-6週間の開発+高いクラッシュリスク
   - 費用対効果が非常に悪い

3. **ユーザー体験の悪化の可能性**
   - OBS起動が遅くなる
   - OBSが不安定になる
   - ユーザーからの「OBSがクラッシュする」報告への対応

##### オプションDが適するケース

以下の条件を**すべて**満たす場合のみ：

1. 専用ハードウェア（配信PC ≠ ゲームPC）
2. 配信エンコードはハードウェアエンコーダ（NVENC）で余裕がある
3. 14-26msの改善が必須要件
4. 4-6週間の開発期間が確保できる
5. OBSプラグイン開発の専門知識がある

### 最終推奨

**順位**:

1. **オプションB（2バイナリ分離）** ⭐⭐⭐ 推奨

   - 実装難易度: ★★☆☆☆
   - 開発期間: 2-3週間
   - 安定性: 高
   - 配信への影響: なし

2. **オプションC（obs-websocket継続）** ⭐⭐

   - 実装難易度: ★☆☆☆☆
   - 開発期間: 1-2週間
   - 安定性: 高
   - 14-26msのオーバーヘッド許容

3. **オプションA（1バイナリ、画像転送あり）** ⭐

   - 実装難易度: ★★★★☆
   - オプションBと大差ない複雑さで、デメリットが多い

4. **オプションD（All-in-One）** ❌ 非推奨
   - 実装難易度: ★★★★★
   - 配信品質リスクが高い
   - 開発コストに見合わない

---

## 設計決定事項

### 1. OBSプラグインの実装について

**決定**: ❌ **OBSプラグインは実装しない**

**採用する方式**: obs-websocket経由での画像取得（PoCと同様）

**理由**:

1. **実証済みの安定性**

   - PoCで十分な性能を確認済み
   - 14-26msのオーバーヘッドは0.25秒間隔で許容範囲

2. **開発コストの削減**

   - OBSプラグイン開発: 4-6週間
   - obs-websocket継続: 1-2週間
   - 約3-4週間の開発期間短縮

3. **安定性の優先**

   - OBSプロセスから独立して動作
   - クラッシュリスクなし
   - デバッグが容易

4. **実装の簡潔さ**
   - Rustの`obws`クレートは成熟している
   - PoCからの移植が容易

**アーキテクチャ**:

```
OBS
  ↓ (obs-websocket)
Rust実行ファイル
  ├── 画像取得（obws クレート）
  ├── Base64デコード
  ├── PNGデコード
  ├── CNN推論（ONNX Runtime）
  ├── StateManager
  └── HTTPサーバー（axum）
```

**将来の拡張性**: レイテンシー改善が必要になった場合、後からOBSプラグインを追加実装する選択肢を残す。

---

### 2. CNNモデル形式について

**決定**: ✅ **ONNX形式を採用**

**採用する方式**: PyTorchで学習 → ONNX変換 → Rustで推論

**理由**:

1. **開発コストの最小化**

   - ONNX変換スクリプト実装: 1-2日
   - Rustネイティブ実装: 4-6日
   - tch-rs（PyTorch C++ API）: 複雑な依存関係

2. **学習環境は引き続きPyTorch**

   - PoCの学習スクリプトを継続使用
   - エコシステムが成熟（豊富なツール、ドキュメント、コミュニティ）
   - 開発効率が高い（実験・試行錯誤が容易）

3. **推論の簡素化**

   - ONNX Runtimeは高性能で安定
   - Rustバインディング（`ort`クレート）が成熟
   - モデル構造変更時も再変換のみで対応

4. **配布の簡便さ**
   - エンドユーザーはPython環境不要
   - 単一バイナリ + ONNXファイルのみ
   - 依存関係を最小化

**実装内容**:

1. **新規追加**: ONNX変換スクリプト (`convert_to_onnx.py`)

   ```bash
   # PyTorch学習後に実行
   python scripts/convert_to_onnx.py \
     --input artifacts/models/victory_classifier.pth \
     --output models/victory_classifier.onnx
   ```

2. **学習環境**: PoCのまま変更なし

   - `build_dataset.py`: データセット構築
   - `train_classifier.py`: モデル学習
   - 将来的にはスクリプトを整理・リファクタリング予定

3. **推論環境**: Rust + ONNX Runtime
   ```rust
   use ort::{Session, SessionBuilder};
   let session = Session::builder()?
       .with_model_from_file("models/victory_classifier.onnx")?;
   ```

**アーキテクチャ**:

```
[開発者環境]
Python + PyTorch
  ├── データセット構築（build_dataset.py）
  ├── モデル学習（train_classifier.py）
  └── ONNX変換（convert_to_onnx.py）✨ 新規
        ↓
     victory_classifier.onnx
     victory_classifier.label_map.json

[エンドユーザー環境]
Rust実行ファイル
  └── ONNX Runtime推論
        ├── .onnxファイル読み込み
        ├── label_map.json読み込み
        └── 高速推論（PyTorchと同等の精度）
```

**将来の展望**:

- **短期（今回）**: PyTorch学習 + ONNX推論
- **中期（1-2年後）**: 学習スクリプトの整理・リファクタリング
- **長期（3-5年後）**: burn v1.0リリース後に学習のRust化を再検討

**却下した選択肢**:

- ❌ **tch-rs（PyTorch C++ API）**: 複雑な依存関係、配布が困難
- ❌ **Rustネイティブ実装（burn、candle）**: 実装コスト4-6日、学習のRust化は9-14日

---

### 3. ブラウザへのリアルタイム通知方法

**決定**: ✅ **SSE (Server-Sent Events) を採用**

**採用する方式**: axum の SSE サポートを利用

**理由**:

1. **単方向通信で十分**

   - カウンター表示は読み取り専用（ブラウザー→サーバーの通信不要）
   - サーバー→ブラウザーのイベント通知のみ

2. **OBSブラウザーソースで完全サポート**

   - OBSブラウザーソースはChromium Embedded Framework (CEF) ベース
   - EventSource API（SSE標準API）が利用可能
   - 特別な対応なしで動作

3. **自動再接続機能**

   - ネットワーク切断時に自動で再接続
   - エラーハンドリングが容易

4. **軽量でシンプル**

   - WebSocketより実装が簡単
   - HTTP/1.1で動作（プロキシ・ファイアウォール対応）
   - テキストベース（JSONを送信しやすい）

5. **axumで標準サポート**
   - `axum::response::sse::Sse` で簡単に実装
   - keep-alive機能内蔵
   - tokio streamとの統合が容易

**実装内容**:

1. **サーバー側（Rust + axum）**

   ```rust
   use axum::{
       response::sse::{Event, Sse},
       routing::get,
       Router,
   };

   async fn sse_handler(
       State(state): State<AppState>,
   ) -> Sse<impl Stream<Item = Result<Event, Infallible>>> {
       let rx = state.subscribe_to_events();

       let stream = tokio_stream::wrappers::BroadcastStream::new(rx)
           .map(|event| {
               Event::default()
                   .event("counter-update")
                   .json_data(json!({
                       "victories": event.victories,
                       "defeats": event.defeats,
                       "draws": event.draws,
                   }))
           });

       Sse::new(stream).keep_alive(Duration::from_secs(15))
   }
   ```

2. **クライアント側（HTML + JavaScript）**

   ```javascript
   const eventSource = new EventSource("http://localhost:3000/events");

   eventSource.addEventListener("counter-update", (e) => {
     const data = JSON.parse(e.data);
     document.getElementById("victories").textContent = data.victories;
     document.getElementById("defeats").textContent = data.defeats;
     document.getElementById("draws").textContent = data.draws;
   });

   eventSource.onerror = (error) => {
     console.error("SSE connection error:", error);
     // EventSourceは自動再接続
   };
   ```

3. **OBSブラウザーソース設定**
   ```
   URL: http://localhost:3000/
   幅: 1920
   高さ: 1080
   ローカルファイル: チェックなし
   ```

**アーキテクチャ**:

```
Rust HTTPサーバー（axum）
  ├── GET /events
  │     └── SSEストリーム（counter-update イベント）
  └── GET /
        └── HTMLファイル（カウンター表示UI）

        ↓ SSE (HTTP/1.1)

OBSブラウザーソース
  └── EventSource API
        └── リアルタイムカウンター更新
```

**データフロー**:

```
StateManager（勝敗判定）
  ↓ イベント発火
tokio::sync::broadcast
  ↓ 購読
SSEハンドラー
  ↓ Event::json_data
ブラウザー（EventSource）
  ↓ counter-update イベント
DOM更新（カウンター表示）
```

**メリット**:

- ローカルホスト接続なのでHTTPSで十分（HTTPSは不要）
- CORS対策不要（同一オリジン）
- Keep-Alive（15秒間隔）で接続維持
- JSONベースでデータ構造の変更が容易

**却下した選択肢**:

- ❌ **WebSocket**: 双方向通信不要（オーバースペック）、実装が複雑
- ❌ **ポーリング**: リアルタイム性が低い、サーバー負荷が高い

---

### 4. フロントエンド配置方法とエンドポイント設計

**決定**: ✅ **Svelte + CSS変数カスタマイズ + ハイブリッド配置**

**採用する方式**: 2段階のカスタマイズ性を提供

**理由**:

1. **リッチなアニメーション実装が容易**

   - `tweened`ストアでカウントアップアニメーション
   - `transition:`ディレクティブでエフェクト追加
   - PoCより視覚的に洗練されたUIを実現

2. **段階的なカスタマイズ性**

   - レベル0（デフォルト）: 何もしない
   - レベル1（CSS編集）: `custom.css`でスタイル変更
   - レベル2（完全カスタマイズ）: APIを使って独自UI実装

3. **軽量かつ高性能**

   - Svelteコンパイル後: 約15KB（gzip後 5KB）
   - ランタイムなし、純粋なJavaScript
   - 高速な初期ロード、滑らかなアニメーション

4. **ハイブリッド配置で柔軟性確保**
   - デフォルト: バイナリー組み込み（即座に動作）
   - カスタマイズ: 外部ファイル優先読み込み

**エンドポイント設計**:

| エンドポイント    | メソッド | 用途                               | 標準UI | 独自UI |
| ----------------- | -------- | ---------------------------------- | ------ | ------ |
| `/`               | GET      | 標準UI提供（Svelte）               | ✅     | -      |
| `/custom.css`     | GET      | カスタマイズ用CSS（オプション）    | ✅     | -      |
| `/events`         | GET      | リアルタイム通知（SSE）            | ✅     | ✅     |
| `/api/status`     | GET      | 現在の状態取得（JSON）             | ❌     | ✅     |
| `/api/initialize` | POST     | 勝敗数初期化（ブラウザーから送信） | ✅     | ✅     |
| `/api/adjust`     | POST     | 勝敗数調整（手動増減）             | ✅     | ✅     |

**データフロー**:

```
[ブラウザー起動]
  ↓
localStorage から前回の状態を読み込み
  ↓
POST /api/initialize（サーバーに状態を復元）
  ↓
GET /events（SSE接続、リアルタイム更新受信）
  ↓
イベント受信時: UI更新 + localStorage保存

[勝敗判定時]
StateManager（Rust）
  ↓ イベント発火
tokio::sync::broadcast
  ↓ 全接続クライアントに配信
SSE → ブラウザー
  ↓
UI更新（Svelteアニメーション）+ localStorage保存
```

**永続化戦略**:

- **サーバー側**: ステートレス（イベントログのみ保存）
- **ブラウザー側**: localStorage または IndexedDB で状態を永続化
- **復元**: ブラウザー起動時に `/api/initialize` で状態をサーバーに送信

**実装内容**:

1. **標準UI（Svelte）**

   ```svelte
   <script>
     import { tweened } from 'svelte/motion';
     import { cubicOut } from 'svelte/easing';
     import { onMount } from 'svelte';

     let victories = tweened(0, { duration: 1000, easing: cubicOut });
     let defeats = tweened(0, { duration: 1000, easing: cubicOut });
     let draws = tweened(0, { duration: 1000, easing: cubicOut });

     // 初期化
     onMount(async () => {
       const saved = localStorage.getItem('counter_state');
       if (saved) {
         const data = JSON.parse(saved);
         await fetch('/api/initialize', {
           method: 'POST',
           headers: { 'Content-Type': 'application/json' },
           body: JSON.stringify(data),
         });
       }

       // SSE接続
       const eventSource = new EventSource('/events');
       eventSource.addEventListener('counter-update', (e) => {
         const data = JSON.parse(e.data);
         victories.set(data.victories);
         defeats.set(data.defeats);
         draws.set(data.draws);
         localStorage.setItem('counter_state', JSON.stringify(data));
       });
     });
   </script>

   <div class="counter-grid">
     <div class="victory">
       <div class="label">Victory</div>
       <div class="value">{Math.floor($victories)}</div>
     </div>
     <!-- ... -->
   </div>

   <style>
     .counter-grid {
       --victory-color: #4caf50;
       --defeat-color: #f44336;
       --draw-color: #ff9800;
       --font-size: 64px;
     }
     /* 外部 custom.css で上書き可能 */
   </style>
   ```

2. **カスタマイズ用CSS（`custom.css`）**

   ```css
   /* ユーザーが編集可能 */
   .counter-grid {
     --font-size: 96px;
     --victory-color: #00ff00;
     background: linear-gradient(45deg, #000, #333);
   }

   .value {
     text-shadow: 0 0 10px currentColor;
   }
   ```

3. **Rustサーバー側（ハイブリッド配置）**

   ```rust
   async fn serve_ui() -> Html<String> {
       // 外部ファイル優先（カスタマイズ版）
       if let Ok(html) = tokio::fs::read_to_string("templates/counter.html").await {
           return Html(html);
       }

       // デフォルト（バイナリー組み込み）
       Html(include_str!("../dist/index.html").to_string())
   }

   async fn serve_custom_css() -> Result<Response, StatusCode> {
       if let Ok(css) = tokio::fs::read_to_string("templates/custom.css").await {
           return Ok(Response::builder()
               .header("Content-Type", "text/css")
               .body(css.into())
               .unwrap());
       }
       Ok(Response::builder()
           .header("Content-Type", "text/css")
           .body("".into())
           .unwrap())
   }
   ```

**配置構成**:

```
victory-detector/
├── victory-detector.exe              # Rustバイナリ（標準UI組み込み）
├── templates/                        # カスタマイズ用（オプション）
│   ├── counter.html                  # 完全カスタマイズ版UI
│   └── custom.css                    # CSS変数上書き
├── src-templates/                    # Svelteソースファイル
│   ├── Counter.svelte
│   ├── package.json
│   └── README.md                     # ビルド方法
└── README.md
```

**カスタマイズレベルの詳細**:

| レベル          | 方法                         | 対象ユーザー         | 難易度 | 必要スキル     |
| --------------- | ---------------------------- | -------------------- | ------ | -------------- |
| レベル0         | 何もしない                   | 全員                 | なし   | なし           |
| レベル1（CSS）  | `custom.css` を配置して編集  | CSS知識あり          | 低     | CSS            |
| レベル2（UI）   | `/events` APIで独自UI実装    | Web開発者            | 中〜高 | HTML/JS/CSS    |
| レベル3（上級） | Svelteソース編集して再ビルド | フロントエンド開発者 | 高     | Svelte/Node.js |

**メリット**:

- **即座に動作**: バイナリー単体で完結
- **簡易カスタマイズ**: CSSのみで外観変更可能
- **完全カスタマイズ**: APIを使って独自UI実装可能
- **デバッグ容易**: `/api/status` でコマンドラインから状態確認

**`GET /api/status` の用途**:

- デバッグ: `curl http://localhost:3000/api/status`
- 独自UI実装時の初期値取得（SSE不要の場合）
- 他ツール連携（OBSスクリプトなど）
- RESTful設計の完全性

**却下した選択肢**:

- ❌ **バニラJS**: リッチなアニメーション実装が手間
- ❌ **Vue（CDN版）**: ネットワーク依存、サイズ大（300KB）
- ❌ **バイナリー組み込みのみ**: カスタマイズ不可
- ❌ **外部HTMLファイルのみ**: デフォルトで動作しない

---

## PNGデコードの最適化検討

### CV2（OpenCV）とは

**CV2 = OpenCV (Open Source Computer Vision Library)**

- コンピュータビジョン用のC++ライブラリ
- Pythonでは`cv2`モジュールとしてバインディングを使用
- 画像処理の業界標準ライブラリ
- 画像のデコード、エンコード、変換、フィルタリングなどの機能

**PoCでの使用箇所**:

```python
# Base64デコード後
png_bytes = base64.b64decode(base64_data)
np_data = np.frombuffer(png_bytes, np.uint8)
image = cv2.imdecode(np_data, cv2.IMREAD_COLOR)  # ← PNGデコード（10-20ms）
```

### PNGデコードが遅い理由

**PNGの特性**:

- **可逆圧縮**フォーマット（画質劣化なし）
- DEFLATE圧縮アルゴリズム（ZIP系）
- 圧縮率は高いが、デコードに計算コストがかかる

**1920×1080 PNG の処理時間**:

| 処理                | 時間        | 備考                            |
| ------------------- | ----------- | ------------------------------- |
| Base64 デコード     | 1-2ms       | 単純なバイナリ変換              |
| PNG デコード（cv2） | **10-20ms** | DEFLATE圧縮解除 + RGB変換       |
| Tensor 変換         | 1-2ms       | numpy → PyTorch/ONNX            |
| **合計**            | **12-24ms** | 0.25秒間隔なので5-10%のオーバー |

### 最適化手法の比較

#### オプション1: JPEGフォーマットに変更

```python
# obs-websocketでJPEG取得
resp = client.get_source_screenshot(
    args.source,
    "jpg",  # PNG → JPEG
    args.screenshot_width,
    args.screenshot_height,
    85,  # JPEG品質（0-100）
)
```

**メリット**:

- デコード時間: **3-5ms**（PNG比で50-75%削減）
- ファイルサイズ: 約1/3-1/5（Base64転送量削減）
- obs-websocketで"jpg"フォーマットをサポート（要確認）

**デメリット**:

- **非可逆圧縮**: 画質劣化がある
- **CNN推論精度への影響**: 勝敗テキストのエッジがぼやける可能性
- 学習データもJPEGで再収集が必要？（検証必要）

**推奨**: 品質85-95なら実用上問題ない可能性が高い。**要検証**。

#### オプション2: WebP フォーマット

```python
resp = client.get_source_screenshot(
    args.source,
    "webp",  # 高速かつ高圧縮
    args.screenshot_width,
    args.screenshot_height,
    90,
)
```

**メリット**:

- PNGより高速（5-8ms）
- JPEGより高画質（同サイズで比較）
- 可逆/非可逆の選択可能

**デメリット**:

- obs-websocketがサポートしているか不明（要確認）
- OpenCVのWebP対応が必要（ビルドオプション依存）

#### オプション3: 解像度を下げる

```python
resp = client.get_source_screenshot(
    args.source,
    "png",
    1280,  # 1920 → 1280
    720,   # 1080 → 720
    -1,
)
```

**メリット**:

- デコード時間: **5-10ms**（画素数が44%減）
- 転送サイズ削減
- PNGのまま使える

**デメリット**:

- CNN推論精度への影響（低解像度で学習が必要）
- クロップ領域の座標調整が必要

**推奨**: クロップ領域 (460, 378, 995, 550) が画面の一部なので、全体を720pにしても十分かもしれない。**要検証**。

#### オプション4: クロップ後の画像のみ取得

obs-websocketに「特定領域のみスクリーンショット」機能があれば理想的：

- スクリーンショット: (460, 378, 995, 550) のみ → 995×550 = 547KB
- デコード時間: **3-5ms**（画素数が72%減）

**課題**: obs-websocket APIが領域指定をサポートしているか不明。

#### オプション5: RustでのOpenCV代替

Rust実装時に異なるデコーダを使う：

| ライブラリ        | デコード速度 | 特徴                               |
| ----------------- | ------------ | ---------------------------------- |
| `opencv-rust`     | 10-20ms      | C++ OpenCVのバインディング（同等） |
| `image` crate     | 15-25ms      | 純粋Rust実装（やや遅い）           |
| `libjpeg-turbo`   | 3-5ms        | SIMD最適化（JPEG専用）             |
| `libpng` (最適化) | 8-12ms       | 最適化ビルドで高速化               |

**結論**: Rustに移植しても、PNGデコードの速度はほぼ同じ。

### 推奨アプローチ

#### Phase 1: 現状維持（PNG 1920×1080）

- PoCで実証済み
- 14-26msのオーバーヘッドは0.25秒間隔なら許容範囲
- まずは動作する実装を完成させる

#### Phase 2: 段階的最適化（必要に応じて）

1. **解像度テスト**（最も安全）

   - 1280×720で推論精度を検証
   - 問題なければデコード時間を50%削減

2. **JPEGテスト**（効果大、リスク中）

   - JPEG品質90-95で精度を検証
   - 学習データへの影響を確認
   - デコード時間を70%削減

3. **WebPテスト**（効果中、対応状況不明）
   - obs-websocketのサポート確認
   - サポートしていれば理想的

### 最適化の効果試算

| 構成                      | デコード時間 | 削減幅 | リスク | 検証労力       |
| ------------------------- | ------------ | ------ | ------ | -------------- |
| **現状（PNG 1920×1080）** | 10-20ms      | -      | なし   | -              |
| PNG 1280×720              | 5-10ms       | 50%    | 低     | 小             |
| JPEG 1920×1080 (品質90)   | 3-5ms        | 70%    | 中     | 中             |
| JPEG 1280×720 (品質90)    | 2-3ms        | 80%    | 中     | 中             |
| WebP 1920×1080            | 5-8ms        | 60%    | 低     | 大（対応確認） |

### obs-websocket API確認事項

obs-websocketのドキュメントで以下を確認する必要があります：

1. `get_source_screenshot` でサポートされる画像フォーマット
   - "png", "jpg", "webp" のどれが使えるか
2. 品質パラメータの範囲（JPEGの場合0-100が一般的）
3. 領域指定スクリーンショットのサポート有無

### 実装プランへの反映

**Phase 2（ONNX推論とOBS連携）** に追加：

1. obs-websocketのフォーマットサポート調査
2. 複数フォーマットでの推論精度テスト
3. 設定で切り替え可能にする（CLI引数またはconfig.toml）

```toml
# config.toml
[obs]
screenshot_format = "png"  # "png" | "jpg" | "webp"
screenshot_width = 1920
screenshot_height = 1080
jpeg_quality = 90  # JPEG使用時のみ
```

### 結論

**現時点での推奨**: PNG 1920×1080のまま実装開始

**将来の最適化**: 必要に応じて解像度削減またはJPEGに切り替え

- 14-26msのオーバーヘッドが問題になった場合のみ
- まずはPoCと同等の動作を確認してから最適化
- 「早すぎる最適化は諸悪の根源」

---

## Rust化による性能改善の分析

### コンポーネント別の性能比較

#### 1. PNGデコード

| 実装          | 実際の処理            | 速度    |
| ------------- | --------------------- | ------- |
| Python (cv2)  | C++ libpng を呼び出し | 10-20ms |
| Rust (opencv) | 同じC++ libpng        | 10-20ms |
| Rust (image)  | 純粋Rust実装          | 15-25ms |

**結論**: ほぼ同じ。どちらも同じC++ライブラリを使用。

#### 2. Base64デコード

| 実装            | 実装詳細                   | 速度    | 改善        |
| --------------- | -------------------------- | ------- | ----------- |
| Python (base64) | C実装                      | 1-2ms   | -           |
| Rust (base64)   | Rust実装（SIMD最適化あり） | 0.5-1ms | **50%削減** |

**改善**: ✅ **0.5-1ms削減**

#### 3. CNN推論

| 実装                | バックエンド | 速度   |
| ------------------- | ------------ | ------ |
| Python (PyTorch)    | C++/CUDA     | 5-15ms |
| Rust (ONNX Runtime) | C++/CUDA     | 5-15ms |

**結論**: ほぼ同じ。どちらも同じC++/CUDAバックエンド。

#### 4. 画像前処理（クロップ、リサイズ、正規化）

| 実装                  | 処理内容                     | 速度  | 改善           |
| --------------------- | ---------------------------- | ----- | -------------- |
| Python (NumPy+cv2)    | C実装 + Pythonオーバーヘッド | 2-3ms | -              |
| Rust (ndarray+opencv) | Rust/C++、ゼロコスト抽象化   | 1-2ms | **30-50%削減** |

**改善**: ✅ **0.5-1ms削減**

理由:

- Pythonインタープリタのオーバーヘッドなし
- メモリレイアウトの最適化
- コンパイル時の最適化（インライン展開など）

#### 5. StateManager（状態管理、イベントログ）

| 実装   | 処理内容                     | 速度      | 改善           |
| ------ | ---------------------------- | --------- | -------------- |
| Python | Pythonインタープリタ         | 1-2ms     | -              |
| Rust   | ネイティブコード、最適化済み | 0.1-0.3ms | **80-90%削減** |

**改善**: ✅ **1-1.7ms削減**

理由:

- ゼロコスト抽象化（`enum`、`match`など）
- メモリアロケーションの最適化
- LLVMコンパイラの最適化

#### 6. JSONシリアライゼーション（イベントログ書き込み）

| 実装              | 実装詳細                 | 速度      | 改善           |
| ----------------- | ------------------------ | --------- | -------------- |
| Python (json)     | C実装                    | 0.5-1ms   | -              |
| Rust (serde_json) | Rust実装（高度な最適化） | 0.1-0.2ms | **70-80%削減** |

**改善**: ✅ **0.4-0.8ms削減**

理由:

- `serde`はゼロコピーシリアライゼーション
- コンパイル時の型情報でコード生成

#### 7. HTTP APIサーバー

| 実装                 | 処理内容                               | スループット  | レイテンシー |
| -------------------- | -------------------------------------- | ------------- | ------------ |
| Python (http.server) | Pythonインタープリタ、シングルスレッド | 100-500 req/s | 10-50ms      |
| Rust (axum)          | 非同期、マルチスレッド、ゼロコスト     | 10,000+ req/s | 0.5-2ms      |

**改善**: ✅ **10-100倍の性能向上**

理由:

- Tokio非同期ランタイム（数万の同時接続）
- ゼロコスト抽象化
- LLVMの最適化

#### 8. メモリ使用量

| 実装   | 内訳                                     | メモリ使用量 |
| ------ | ---------------------------------------- | ------------ |
| Python | インタープリタ + 依存ライブラリ + モデル | 500-800MB    |
| Rust   | ネイティブバイナリ + 依存 + モデル       | 100-200MB    |

**改善**: ✅ **60-75%削減（300-600MB削減）**

理由:

- Pythonインタープリタ不要（~100MB）
- 静的リンク、未使用コードの削除
- より効率的なメモリレイアウト

#### 9. 起動時間

| 実装   | 起動処理                                             | 起動時間 |
| ------ | ---------------------------------------------------- | -------- |
| Python | インタープリタ起動 + ライブラリロード + モデルロード | 3-5秒    |
| Rust   | ネイティブバイナリ実行 + モデルロード                | 0.5-1秒  |

**改善**: ✅ **70-90%削減（2-4.5秒削減）**

### 推論1サイクル全体の性能比較

**Python PoC（現状）**:

```
1サイクル（0.25秒間隔）の処理時間:
├── Base64デコード:        1-2ms
├── PNGデコード:          10-20ms
├── 画像前処理:           2-3ms
├── CNN推論:              5-15ms
├── StateManager:         1-2ms
├── JSONシリアライズ:     0.5-1ms
└── 合計:                 19.5-43ms (平均 30ms)

オーバーヘッド: 30ms / 250ms = 12%
```

**Rust実装（推定）**:

```
1サイクル（0.25秒間隔）の処理時間:
├── Base64デコード:        0.5-1ms    ✅ (-0.5-1ms)
├── PNGデコード:          10-20ms    ➡️ (同等)
├── 画像前処理:           1-2ms      ✅ (-1ms)
├── CNN推論:              5-15ms     ➡️ (同等)
├── StateManager:         0.1-0.3ms  ✅ (-1-1.7ms)
├── JSONシリアライズ:     0.1-0.2ms  ✅ (-0.4-0.8ms)
└── 合計:                 16.7-38.5ms (平均 26ms)

オーバーヘッド: 26ms / 250ms = 10.4%
改善: 13-15%の高速化
```

### Rust化のメリットまとめ

#### パフォーマンス面

| 項目                  | Python    | Rust      | 改善幅             |
| --------------------- | --------- | --------- | ------------------ |
| 1サイクル処理時間     | 平均 30ms | 平均 26ms | **13-15%高速化**   |
| HTTP API応答          | 10-50ms   | 0.5-2ms   | **10-100倍高速化** |
| メモリ使用量          | 500-800MB | 100-200MB | **60-75%削減**     |
| 起動時間              | 3-5秒     | 0.5-1秒   | **70-90%削減**     |
| CPU使用率（アイドル） | 1-3%      | 0.1-0.5%  | **80-90%削減**     |

#### その他のメリット

1. **配布の簡便さ**

   - Python: 実行環境セットアップが必要（Python + 依存ライブラリ）
   - Rust: 単一バイナリ（.exe または ELF）で動作

2. **クロスコンパイル**

   - Python: 各プラットフォームで環境構築が必要
   - Rust: 1つのマシンで全プラットフォーム向けにビルド可能

3. **セキュリティ**

   - Python: ソースコードが見える（.pyc は簡単に逆コンパイル）
   - Rust: ネイティブバイナリ（逆コンパイル困難）

4. **型安全性**

   - Python: 実行時エラー（`AttributeError`, `KeyError` など）
   - Rust: コンパイル時エラー（実行前にバグを発見）

5. **並行性**
   - Python: GIL（Global Interpreter Lock）で真の並列処理が困難
   - Rust: 真の並列処理（複数CPUコアを完全に活用）

### 重要な注意点

**最も時間がかかるのはPNGデコード（10-20ms）**

- これはRust化しても変わらない（同じC++ライブラリを使用）
- **13-15%の高速化**は、PNGデコード以外の部分の最適化による

**真の高速化には**:

1. ✅ Rust化（13-15%改善） - 実装の複雑さは同等
2. ✅ JPEG化（70%改善） - 精度検証が必要
3. ✅ 解像度削減（50%改善） - 精度検証が必要

### 結論

**Rust化だけで13-15%の性能改善が見込めます！**

特に大きな改善：

- ✅ HTTPサーバー: 10-100倍高速化
- ✅ メモリ使用量: 60-75%削減
- ✅ 起動時間: 70-90%削減

**PNGデコードのボトルネックは残る**ものの：

- 配布の簡便さ（単一バイナリ）
- 型安全性（バグの早期発見）
- メモリ効率（ゲームへの影響最小化）

これらの理由から、**Rust実装は十分に価値がある**と言えます。

さらに、将来的にJPEGや解像度削減と組み合わせれば、**50-80%の総合的な高速化**も可能です。

---

## CNNモデルのRust移植方法の詳細検討

### PoCで使用していたモデル形式

**ファイル**: `artifacts/models/victory_classifier.pth`

**形式**: PyTorch固有のシリアライゼーション形式

```python
# 学習時の保存（PoCのtrain_classifier.py）
torch.save({
    'model_state_dict': model.state_dict(),  # 学習済み重み
    'label_map': label_map,                  # クラス名マッピング
}, model_path)

# 推論時の読み込み（VictoryPredictor）
checkpoint = torch.load(model_path)
model.load_state_dict(checkpoint['model_state_dict'])
label_map = checkpoint['label_map']
```

**特徴**:

- ✅ PyTorchで学習してPyTorchで推論するので変換不要
- ✅ モデル構造を柔軟に変更可能（開発時に便利）
- ❌ **PyTorchでしか使えない**（Python + PyTorch必須）
- ❌ Rustで使うには`tch-rs`（PyTorchバインディング）が必要
- ❌ 配布時にlibtorch（C++ライブラリ、数百MB）が必要

**なぜPoCでこの形式を使ったか**:

1. **PoCの目的は「動作検証」**

   - 配布やクロスプラットフォーム対応は考慮していない
   - PyTorchで学習・推論が最も簡単

2. **開発の柔軟性**

   - モデル構造の変更が容易
   - デバッグがしやすい
   - Pythonエコシステムの恩恵

3. **変換の手間を省く**
   - ONNX変換には検証が必要
   - PoCでは不要な手間

### ONNX (Open Neural Network Exchange) とは

**定義**: オープンな機械学習モデル交換フォーマット

**開発元**: Microsoft と Meta（旧Facebook）が2017年に発表

**公式サイト**: https://onnx.ai/

**目的**: 異なるフレームワーク間でモデルを相互運用可能にする

#### ONNXの仕組み

```
学習フレームワーク         ONNX形式              推論環境
──────────────────        ────────            ──────────
PyTorch              →                    →  ONNX Runtime (Python)
TensorFlow           →    .onnx ファイル   →  ONNX Runtime (C++)
scikit-learn         →                    →  ONNX Runtime (Rust)
Keras                →                    →  ONNX Runtime (C#)
                                           →  ONNX Runtime (JavaScript)
                                           →  TensorRT (GPU最適化)
                                           →  CoreML (iOS)
                                           →  DirectML (Windows)
```

**ONNXファイルの中身**:

1. **計算グラフ** - ニューラルネットワークの構造（層、演算子、接続）
2. **学習済み重み** - パラメータの値
3. **メタデータ** - 入力/出力の形状、データ型など

**フォーマット**: Protocol Buffers（Google開発のバイナリ形式）

#### ONNX Runtimeとは

**定義**: Microsoftが開発した高性能な機械学習推論エンジン

**特徴**:

- ✅ クロスプラットフォーム（Windows/Linux/macOS/Android/iOS）
- ✅ 多言語バインディング（Python/C++/C#/Java/Rust/JavaScript）
- ✅ CPU/GPU/様々なアクセラレータ対応
- ✅ 高度な最適化（グラフ最適化、量子化、融合演算など）
- ✅ 本番環境で広く使われている（Microsoft製品、Azure等）

**Rustでの使用**:

```rust
use ort::{Session, GraphOptimizationLevel, ExecutionProvider};

// ONNXモデル読み込み
let session = Session::builder()?
    .with_optimization_level(GraphOptimizationLevel::Level3)?
    .with_execution_providers([ExecutionProvider::CUDA(0)])?  // GPU使用
    .commit_from_file("model.onnx")?;

// 推論
let outputs = session.run(ort::inputs![input_tensor]?)?;
```

### 形式の比較

#### 1. PyTorch .pth (PoCで使用)

**構成**:

```
artifacts/models/victory_classifier.pth
├── モデル構造: Pythonコード（別ファイル model.py）
└── 学習済み重み: state_dict
```

**メリット**:

- ✅ PyTorchで学習・推論がシームレス
- ✅ モデル構造の変更が容易
- ✅ Pythonエコシステムの恩恵
- ✅ デバッグが容易

**デメリット**:

- ❌ PyTorchでしか使えない
- ❌ Rustで使うには`tch-rs` + libtorch（数百MB）が必要
- ❌ 配布が複雑（libtorchの同梱）
- ❌ クロスコンパイルが困難

**汎用性**: ★☆☆☆☆（PyTorchのみ）

#### 2. ONNX .onnx

**構成**:

```
model.onnx
├── 計算グラフ（モデル構造）
└── 学習済み重み
```

**メリット**:

- ✅ フレームワーク非依存（様々な環境で使える）
- ✅ Rustで`ort`クレート（軽量）のみで使える
- ✅ 配布が簡単（.onnxファイル1つ）
- ✅ クロスコンパイル容易
- ✅ ONNX Runtimeの最適化（高速化）
- ✅ 本番環境で広く使われている（実績豊富）

**デメリット**:

- ❌ PyTorchからONNXへの変換作業が必要
- ❌ 変換時にサポートされない演算がある可能性（検証必要）
- ❌ 変換後のモデル精度検証が必要

**汎用性**: ★★★★★（ほぼすべての環境で使える）

#### 3. tch-rs (PyTorch Rustバインディング)

**構成**:

```
Rustアプリ
├── tch-rs クレート
├── libtorch (C++ライブラリ、数百MB)
└── victory_classifier.pth
```

**メリット**:

- ✅ .pthファイルをそのまま使える（変換不要）
- ✅ PyTorchの全機能を利用可能

**デメリット**:

- ❌ libtorchのインストールが必要（配布サイズ増大）
- ❌ クロスコンパイルが困難
- ❌ 配布が複雑（libtorchの同梱、バージョン管理）
- ❌ ビルド設定がプラットフォーム依存

**汎用性**: ★★☆☆☆（Rust + PyTorch環境のみ）

### PyTorchからONNXへの変換

**変換スクリプト例**:

```python
# scripts/convert_to_onnx.py
import torch
from victory_detector.training.model import VictoryClassifier

# モデル読み込み
checkpoint = torch.load("artifacts/models/victory_classifier.pth")
model = VictoryClassifier(num_classes=len(checkpoint['label_map']))
model.load_state_dict(checkpoint['model_state_dict'])
model.eval()

# ダミー入力（推論時の入力サイズと同じ）
# クロップ領域 (995, 550) の画像
dummy_input = torch.randn(1, 3, 550, 995)

# ONNX変換
torch.onnx.export(
    model,
    dummy_input,
    "model.onnx",
    opset_version=14,  # ONNX演算子バージョン
    input_names=['input'],
    output_names=['output'],
    dynamic_axes={
        'input': {0: 'batch_size'},  # バッチサイズ可変
        'output': {0: 'batch_size'}
    }
)

# label_mapを別ファイルで保存
import json
with open("label_map.json", "w") as f:
    json.dump(checkpoint['label_map'], f)

print("✅ ONNX変換完了: model.onnx")
print("✅ ラベルマップ保存: label_map.json")
```

**変換後の検証**:

```python
# 精度検証
import onnxruntime as ort

# ONNX Runtime で推論
session = ort.InferenceSession("model.onnx")
onnx_output = session.run(None, {'input': dummy_input.numpy()})

# PyTorch で推論
pytorch_output = model(dummy_input).detach().numpy()

# 結果比較
diff = np.abs(onnx_output[0] - pytorch_output).max()
print(f"最大誤差: {diff}")  # 1e-6 以下なら問題なし
```

### 配布サイズの比較

| 方式           | 構成要素                       | 配布サイズ    |
| -------------- | ------------------------------ | ------------- |
| PyTorch (.pth) | Python + PyTorch + .pth        | 500MB-1GB以上 |
| ONNX (.onnx)   | 実行ファイル + .onnx           | 10-30MB       |
| tch-rs         | 実行ファイル + libtorch + .pth | 300-500MB     |

### 実装の複雑さ比較

#### ONNX (推奨)

```rust
// 1. 依存クレート
ort = "2"

// 2. 推論コード（シンプル）
let session = Session::builder()?
    .commit_from_file("model.onnx")?;
let outputs = session.run(inputs![input_tensor]?)?;

// 3. 配布
- model.onnx (5-10MB)
- label_map.json (1KB)
```

**実装難易度**: ★★☆☆☆

#### tch-rs

```rust
// 1. 依存クレート + libtorch インストール
tch = "0.13"  // ビルド時にlibtorchが必要

// 2. 推論コード
let mut vs = nn::VarStore::new(Device::Cuda(0));
let model = VictoryClassifier::new(&vs.root(), num_classes);
vs.load("model.pth")?;
let output = model.forward(&input);

// 3. 配布
- 実行ファイル
- libtorch (300-500MB)
- model.pth (5-10MB)
```

**実装難易度**: ★★★★☆

### なぜPoCでONNXを使わなかったか

**理由の整理**:

1. **PoCの目的**

   - 動作検証が目的
   - 配布やクロスプラットフォーム対応は範囲外
   - PyTorchで完結するのが最も簡単

2. **開発の効率**

   - モデル構造の試行錯誤が多い
   - ONNX変換の検証は手間
   - Pythonエコシステムの恩恵（デバッグツール等）

3. **不要な複雑さを避ける**
   - PoCでは「最小限の実装で動作確認」が原則
   - ONNX変換は本実装で行えばよい

### 推奨：ONNX形式

**理由**:

1. ✅ **配布の簡便さ** - .onnxファイル1つ（5-10MB）
2. ✅ **クロスプラットフォーム** - Windows/Linux/macOS対応
3. ✅ **軽量** - libtorch不要
4. ✅ **高速** - ONNX Runtimeの最適化
5. ✅ **実績** - 本番環境で広く使われている
6. ✅ **将来性** - 他の環境（モバイル等）への展開が容易

**デメリットの対処**:

- ❌ 変換作業が必要 → スクリプト化（1回のみ）
- ❌ 精度検証が必要 → 自動テスト化

**PoCとの違い**:

| 項目         | PoC（.pth）            | Rust実装（ONNX）   |
| ------------ | ---------------------- | ------------------ |
| 目的         | 動作検証               | 本番配布           |
| 優先順位     | 開発速度               | 配布の簡便さ、性能 |
| 依存         | Python + PyTorch       | ONNX Runtime のみ  |
| 配布サイズ   | 500MB-1GB              | 10-30MB            |
| ユーザー環境 | Python環境セットアップ | 単一バイナリ実行   |

### 結論

**採用する方式**: ONNX形式 ⭐推奨

**次のステップ**:

1. PyTorchモデルをONNXに変換するスクリプト作成
2. 変換後の精度検証
3. Rustで`ort`クレートを使った推論実装
4. PoCと同等の精度を確認

---

## Rustネイティブ実装の詳細

### 「Rustネイティブ実装」とは何か？

**定義**: PyTorchモデルをRustのMLフレームワークで完全に再実装すること

**誤解されがちなポイント**:

- ❌ 独自フォーマットを定義するわけではない
- ✅ RustのMLフレームワーク（burn、candle等）でモデルを書き直す

### 実装の流れ

#### ステップ1: モデル構造をRustで再実装

**Python (PyTorch)** - PoCの実装:

```python
# packages/obs-victory-counter/victory-detector/src/victory_detector/training/model.py
import torch.nn as nn

class VictoryClassifier(nn.Module):
    def __init__(self, num_classes=5):
        super().__init__()
        # Conv層1
        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)
        self.bn1 = nn.BatchNorm2d(32)
        self.pool1 = nn.MaxPool2d(2, 2)

        # Conv層2
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.bn2 = nn.BatchNorm2d(64)
        self.pool2 = nn.MaxPool2d(2, 2)

        # Conv層3
        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)
        self.bn3 = nn.BatchNorm2d(128)
        self.pool3 = nn.MaxPool2d(2, 2)

        # 適応型プーリング
        self.adaptive_pool = nn.AdaptiveAvgPool2d((4, 4))

        # 全結合層
        self.fc = nn.Linear(128 * 4 * 4, num_classes)

    def forward(self, x):
        x = self.pool1(F.relu(self.bn1(self.conv1(x))))
        x = self.pool2(F.relu(self.bn2(self.conv2(x))))
        x = self.pool3(F.relu(self.bn3(self.conv3(x))))
        x = self.adaptive_pool(x)
        x = x.view(x.size(0), -1)
        x = self.fc(x)
        return x
```

**Rust (burn)** - 同じモデルの再実装:

```rust
use burn::{
    module::Module,
    nn::{
        conv::{Conv2d, Conv2dConfig},
        pool::{MaxPool2d, MaxPool2dConfig, AdaptiveAvgPool2d, AdaptiveAvgPool2dConfig},
        BatchNorm, BatchNormConfig, Linear, LinearConfig,
    },
    tensor::{backend::Backend, Tensor},
};

#[derive(Module, Debug)]
pub struct VictoryClassifier<B: Backend> {
    conv1: Conv2d<B>,
    bn1: BatchNorm<B, 2>,
    pool1: MaxPool2d,

    conv2: Conv2d<B>,
    bn2: BatchNorm<B, 2>,
    pool2: MaxPool2d,

    conv3: Conv2d<B>,
    bn3: BatchNorm<B, 2>,
    pool3: MaxPool2d,

    adaptive_pool: AdaptiveAvgPool2d,

    fc: Linear<B>,
}

impl<B: Backend> VictoryClassifier<B> {
    pub fn new(num_classes: usize, device: &B::Device) -> Self {
        Self {
            conv1: Conv2dConfig::new([3, 32], [3, 3])
                .with_padding([1, 1])
                .init(device),
            bn1: BatchNormConfig::new(32).init(device),
            pool1: MaxPool2dConfig::new([2, 2]).init(),

            conv2: Conv2dConfig::new([32, 64], [3, 3])
                .with_padding([1, 1])
                .init(device),
            bn2: BatchNormConfig::new(64).init(device),
            pool2: MaxPool2dConfig::new([2, 2]).init(),

            conv3: Conv2dConfig::new([64, 128], [3, 3])
                .with_padding([1, 1])
                .init(device),
            bn3: BatchNormConfig::new(128).init(device),
            pool3: MaxPool2dConfig::new([2, 2]).init(),

            adaptive_pool: AdaptiveAvgPool2dConfig::new([4, 4]).init(),

            fc: LinearConfig::new(128 * 4 * 4, num_classes).init(device),
        }
    }

    pub fn forward(&self, x: Tensor<B, 4>) -> Tensor<B, 2> {
        let x = self.conv1.forward(x);
        let x = self.bn1.forward(x);
        let x = x.relu();
        let x = self.pool1.forward(x);

        let x = self.conv2.forward(x);
        let x = self.bn2.forward(x);
        let x = x.relu();
        let x = self.pool2.forward(x);

        let x = self.conv3.forward(x);
        let x = self.bn3.forward(x);
        let x = x.relu();
        let x = self.pool3.forward(x);

        let x = self.adaptive_pool.forward(x);
        let x = x.flatten(1, 3);

        self.fc.forward(x)
    }
}
```

#### ステップ2: 学習済み重みの移行

**課題**: PyTorchの`.pth`からRust形式への変換

**アプローチ1: 中間形式（JSON/バイナリ）経由**

```python
# scripts/export_weights_for_rust.py
import torch
import json
import numpy as np

# PyTorchモデル読み込み
checkpoint = torch.load("model.pth")
state_dict = checkpoint['model_state_dict']

# 重みを辞書形式に変換
weights = {}
for name, param in state_dict.items():
    weights[name] = param.cpu().numpy().tolist()

# JSON出力（小規模モデル向け）
with open("weights.json", "w") as f:
    json.dump(weights, f)

# またはバイナリ形式（大規模モデル向け）
np.savez("weights.npz", **{k: v.cpu().numpy() for k, v in state_dict.items()})
```

```rust
// Rustで読み込み
use serde_json::Value;
use std::fs;

let weights_json = fs::read_to_string("weights.json")?;
let weights: HashMap<String, Vec<Vec<f32>>> = serde_json::from_str(&weights_json)?;

// モデルのパラメータにロード
model.conv1.weight = Tensor::from_data(weights["conv1.weight"], device);
model.conv1.bias = Tensor::from_data(weights["conv1.bias"], device);
// ... (すべての層について繰り返し)
```

**アプローチ2: burnの重み形式**

burnには独自の重み保存形式があります：

```rust
// burnでの重み保存（学習後）
model.save_file("model.burn", &CompactRecorder::new())?;

// 読み込み
let model = VictoryClassifier::load_file("model.burn", &CompactRecorder::new(), device)?;
```

**問題**: PyTorchからburnの形式への直接変換ツールは存在しない → 手動マッピングが必要

#### ステップ3: 推論実行

```rust
use burn::tensor::{Tensor, Data};

// 画像データをTensorに変換
let image_data = Data::from(image_array);  // [3, 550, 995]
let input = Tensor::<B, 4>::from_data(image_data.unsqueeze(), device);

// 推論
let output = model.forward(input);

// Softmax + argmax
let probabilities = output.softmax(1);
let predicted_class = probabilities.argmax(1);
```

### RustのMLフレームワーク比較

#### 1. burn

**公式サイト**: https://burn.dev/

**特徴**:

- ✅ Rust純粋実装
- ✅ モジュール設計が優秀（PyTorchライク）
- ✅ 複数バックエンド対応（CPU/CUDA/WebGPU）
- ❌ まだ発展途上（v0.15時点）
- ❌ PyTorchからの移行ツールなし

**コード例**:

```rust
use burn::prelude::*;

#[derive(Module, Debug)]
pub struct MyModel<B: Backend> {
    conv: Conv2d<B>,
    linear: Linear<B>,
}
```

#### 2. candle

**公式サイト**: https://github.com/huggingface/candle

**開発元**: Hugging Face

**特徴**:

- ✅ Hugging Faceが開発（信頼性）
- ✅ 軽量・高速
- ✅ SafeTensors形式サポート
- ❌ APIがまだ不安定
- ❌ ドキュメントが少ない

**コード例**:

```rust
use candle_core::{Tensor, Device};
use candle_nn::{conv2d, linear, Module};

let conv = conv2d(3, 32, 3, Default::default())?;
let output = conv.forward(&input)?;
```

#### 3. tract

**公式サイト**: https://github.com/sonos/tract

**開発元**: Sonos

**特徴**:

- ✅ **ONNXモデルをRustで実行**（これが重要！）
- ✅ 本番環境での実績
- ✅ 高度な最適化
- ❌ 低レベルAPI（使いにくい）

**注意**: tractは「Rustネイティブ」というより「ONNXをRustで実行」なので、実質的にONNX Runtimeの代替

### Rustネイティブ実装のメリット・デメリット

#### メリット

1. **完全なRust実装**

   - 外部C++ライブラリ不要
   - 依存が最小限

2. **バイナリサイズが最小**

   - ONNX Runtime: 10-30MB
   - Rustネイティブ: 5-10MB（推定）

3. **理論上は最速**

   - Rustコンパイラの最適化
   - ゼロコスト抽象化

4. **完全なコントロール**
   - モデルの内部動作を完全に理解
   - カスタマイズが容易

#### デメリット

1. **モデルの再実装が必要**

   - 28行のPythonコード → 100行以上のRustコード
   - 実装の正しさを保証する必要

2. **学習済み重みの移行作業**

   - PyTorchの.pth → Rust形式への手動変換
   - 層ごとのマッピングが必要
   - エラーが混入しやすい

3. **精度検証が困難**

   - PyTorchとRust実装で結果が完全一致するか検証
   - 数値誤差の許容範囲の判断

4. **学習もRustで行う必要**

   - burnで学習パイプラインを実装
   - PyTorchほど成熟していない
   - データローダー、最適化手法等を自前実装

5. **フレームワークの成熟度**

   - burn: v0.15（まだベータ）
   - candle: 開発中
   - PyTorch: v2.0以上（安定）

6. **エコシステムの不足**
   - デバッグツールが少ない
   - 可視化ツールが少ない
   - コミュニティが小さい

### 実装工数の比較

| タスク     | ONNX                      | Rustネイティブ          |
| ---------- | ------------------------- | ----------------------- |
| モデル変換 | 1-2時間（スクリプト作成） | 2-3日（再実装+検証）    |
| 重み移行   | 自動                      | 1-2日（手動マッピング） |
| 精度検証   | 自動                      | 1日（手動比較）         |
| 推論実装   | 0.5日                     | 0.5日                   |
| **合計**   | **1-2日**                 | **4-6日**               |

### 推奨しない理由

**Rustネイティブ実装は以下の理由で推奨しません**:

1. **実装コストが高すぎる**

   - 4-6日の追加作業
   - エラー混入のリスク

2. **メリットが限定的**

   - バイナリサイズ: 5-10MB削減（30MB → 20MB程度）
   - 速度: ほぼ同等（どちらも最適化済み）

3. **将来の保守が困難**

   - モデル構造変更時にRustコードも修正が必要
   - PyTorchで学習 → Rustで推論の二重管理

4. **フレームワークが未成熟**
   - burn/candleはまだ発展途上
   - 本番環境での実績が少ない

### ONNXとの比較まとめ

| 項目           | ONNX ⭐推奨                    | Rustネイティブ               |
| -------------- | ------------------------------ | ---------------------------- |
| 実装工数       | 1-2日                          | 4-6日                        |
| バイナリサイズ | 10-30MB                        | 5-10MB                       |
| 速度           | 高速（ONNX Runtime最適化）     | 理論上は最速（実測ほぼ同等） |
| 保守性         | 高い（PyTorch → ONNX自動変換） | 低い（手動同期）             |
| 実績           | 豊富（Microsoft等）            | 限定的                       |
| リスク         | 低い                           | 中〜高（実装バグの可能性）   |

### 結論

**Rustネイティブ実装は採用しない**

**理由**:

- 実装コストが高い（4-6日）
- メリットが限定的（5-10MBのサイズ削減のみ）
- 将来の保守が困難
- フレームワークが未成熟

**ONNX形式を採用**することで、実装コストを大幅に削減しつつ、十分な性能と配布の簡便さを実現できます。

---

## 学習と推論の分離：ONNXのワークフロー

### 重要な原則：学習はPyTorchのまま

**誤解されがちなポイント**:

- ❌ ONNX採用 = 学習もONNXで行う必要がある
- ✅ **正しい**: 学習はPyTorch、推論のみONNX Runtime

**ONNXは推論専用フォーマット**:

- ONNX形式では学習（訓練）はできません
- あくまで「学習済みモデルの配布・実行」用
- 学習は引き続きPyTorch（またはTensorFlow等）で行います

### 開発・運用のワークフロー

#### PoCのワークフロー（現状）

```
┌─────────────────────────────────────┐
│ Python環境（開発者）                 │
│                                     │
│ 1. データセット構築                  │
│    └─ build_dataset.py              │
│                                     │
│ 2. モデル学習（PyTorch）             │
│    └─ train_classifier.py           │
│    └─ 出力: victory_classifier.pth  │
│                                     │
│ 3. 推論（PyTorch）                   │
│    └─ VictoryPredictor (PyTorch)    │
│    └─ victory_classifier.pth 使用   │
└─────────────────────────────────────┘
```

#### Rust実装のワークフロー（ONNX採用）

```
┌─────────────────────────────────────┐
│ Python環境（開発者・学習専用）       │
│                                     │
│ 1. データセット構築                  │
│    └─ build_dataset.py              │
│    └─ PoCと同じスクリプト            │
│                                     │
│ 2. モデル学習（PyTorch）             │
│    └─ train_classifier.py           │
│    └─ PoCと同じスクリプト            │
│    └─ 出力: victory_classifier.pth  │
│                                     │
│ 3. ONNX変換（新規）                  │
│    └─ convert_to_onnx.py            │
│    └─ 入力: victory_classifier.pth  │
│    └─ 出力: victory_classifier.onnx │
└─────────────────────────────────────┘
                  ↓
┌─────────────────────────────────────┐
│ Rust環境（エンドユーザー・推論専用） │
│                                     │
│ 4. 推論（ONNX Runtime）              │
│    └─ VictoryPredictor (Rust)       │
│    └─ victory_classifier.onnx 使用  │
└─────────────────────────────────────┘
```

### PoCからの変更点

| コンポーネント   | PoC                   | Rust実装              | 変更の有無  |
| ---------------- | --------------------- | --------------------- | ----------- |
| データセット構築 | `build_dataset.py`    | `build_dataset.py`    | ✅ 変更なし |
| モデル学習       | `train_classifier.py` | `train_classifier.py` | ✅ 変更なし |
| モデル変換       | -                     | `convert_to_onnx.py`  | ✨ 新規追加 |
| 推論実装         | PyTorch (Python)      | ONNX Runtime (Rust)   | 🔄 言語変更 |

### 学習環境の保持理由

**PyTorchで学習を続けるべき理由**:

1. **Pythonエコシステムの恩恵**

   - Jupyter Notebook での対話的開発
   - matplotlib/seaborn での可視化
   - pandas でのデータ分析
   - 豊富なデバッグツール

2. **PyTorchの成熟度**

   - 安定したAPI
   - 豊富なドキュメント
   - 大規模なコミュニティ
   - 最新の研究成果の実装

3. **開発速度**

   - モデル構造の試行錯誤が容易
   - データ拡張・前処理の実装が簡単
   - 学習過程の可視化が容易

4. **PoCからの継続性**
   - 既存の学習スクリプトをそのまま使える
   - 学習済みモデル（.pth）も再利用可能
   - 学習ノウハウを引き継げる

### 実装すべき新規スクリプト

#### convert_to_onnx.py（新規）

```python
"""
PyTorchモデルをONNX形式に変換するスクリプト

使い方:
    python scripts/convert_to_onnx.py \
        --input artifacts/models/victory_classifier.pth \
        --output models/victory_classifier.onnx
"""

import argparse
import torch
import json
from pathlib import Path
from victory_detector.training.model import VictoryClassifier

def convert_to_onnx(
    input_path: Path,
    output_path: Path,
    opset_version: int = 14
):
    """PyTorchモデルをONNX形式に変換"""

    # 1. PyTorchモデル読み込み
    print(f"[1/4] PyTorchモデル読み込み: {input_path}")
    checkpoint = torch.load(input_path, map_location='cpu')
    num_classes = len(checkpoint['label_map'])

    model = VictoryClassifier(num_classes=num_classes)
    model.load_state_dict(checkpoint['model_state_dict'])
    model.eval()

    # 2. ダミー入力作成（クロップ後の画像サイズ）
    # クロップ領域 (460, 378, 995, 550) → 995x550の画像
    print("[2/4] ダミー入力作成: (1, 3, 550, 995)")
    dummy_input = torch.randn(1, 3, 550, 995)

    # 3. ONNX変換
    print(f"[3/4] ONNX変換中...")
    output_path.parent.mkdir(parents=True, exist_ok=True)

    torch.onnx.export(
        model,
        dummy_input,
        str(output_path),
        export_params=True,
        opset_version=opset_version,
        do_constant_folding=True,  # 最適化
        input_names=['input'],
        output_names=['output'],
        dynamic_axes={
            'input': {0: 'batch_size'},
            'output': {0: 'batch_size'}
        }
    )

    # 4. label_map保存（別ファイル）
    label_map_path = output_path.with_suffix('.label_map.json')
    print(f"[4/4] ラベルマップ保存: {label_map_path}")
    with open(label_map_path, 'w') as f:
        json.dump(checkpoint['label_map'], f, indent=2)

    print(f"\n✅ 変換完了:")
    print(f"  - ONNXモデル: {output_path}")
    print(f"  - ラベルマップ: {label_map_path}")

    # 精度検証
    verify_conversion(model, output_path, dummy_input)


def verify_conversion(pytorch_model, onnx_path: Path, dummy_input):
    """変換後の精度検証"""
    import onnxruntime as ort
    import numpy as np

    print("\n[検証] 変換精度チェック中...")

    # PyTorch推論
    with torch.no_grad():
        pytorch_output = pytorch_model(dummy_input).numpy()

    # ONNX Runtime推論
    session = ort.InferenceSession(str(onnx_path))
    onnx_output = session.run(
        None,
        {'input': dummy_input.numpy()}
    )[0]

    # 誤差計算
    max_diff = np.abs(pytorch_output - onnx_output).max()
    mean_diff = np.abs(pytorch_output - onnx_output).mean()

    print(f"  - 最大誤差: {max_diff:.2e}")
    print(f"  - 平均誤差: {mean_diff:.2e}")

    if max_diff < 1e-5:
        print("  ✅ 変換精度良好（誤差 < 1e-5）")
    elif max_diff < 1e-4:
        print("  ⚠️  変換精度やや低い（誤差 < 1e-4）")
    else:
        print("  ❌ 変換精度不足（誤差 >= 1e-4）")
        raise ValueError("ONNX変換の精度が低すぎます")


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--input",
        type=Path,
        default=Path("artifacts/models/victory_classifier.pth"),
        help="入力PyTorchモデル (.pth)"
    )
    parser.add_argument(
        "--output",
        type=Path,
        default=Path("models/victory_classifier.onnx"),
        help="出力ONNXモデル (.onnx)"
    )
    parser.add_argument(
        "--opset-version",
        type=int,
        default=14,
        help="ONNX opsetバージョン"
    )

    args = parser.parse_args()
    convert_to_onnx(args.input, args.output, args.opset_version)
```

### 開発フロー

#### モデル改善時の作業フロー

```bash
# 1. データセット構築（PoCと同じ）
cd packages/obs-victory-counter/victory-detector
python scripts/build_dataset.py

# 2. モデル学習（PoCと同じ）
python scripts/train_classifier.py

# 3. ONNX変換（新規、学習後に1回実行）
python scripts/convert_to_onnx.py

# 4. Rust実装で推論（エンドユーザー環境）
cd packages/victory-detector-rs
cargo run
```

#### モデル構造変更時

**学習側（Python）**:

```python
# victory_detector/training/model.py
class VictoryClassifier(nn.Module):
    def __init__(self, num_classes=5):
        # ... 新しい層を追加
        self.new_layer = nn.Conv2d(128, 256, 3)
```

**推論側（Rust）**: 変更不要！

- ONNXファイルに構造が含まれているため
- 再変換すれば自動的に新構造に対応

### 学習環境のセットアップ

**開発者（学習を行う人）**:

```bash
# Python環境（PoCと同じ）
cd packages/obs-victory-counter/victory-detector
uv sync

# 学習
uv run python scripts/train_classifier.py

# ONNX変換
uv run python scripts/convert_to_onnx.py
```

**エンドユーザー（推論のみ）**:

```bash
# Rust実行ファイルのみ
./victory-detector.exe

# 必要なファイル:
# - victory-detector.exe
# - models/victory_classifier.onnx
# - models/victory_classifier.label_map.json
```

### まとめ

**学習と推論の分離による利点**:

| 側面   | 学習（PyTorch）   | 推論（ONNX Runtime） |
| ------ | ----------------- | -------------------- |
| 環境   | Python + PyTorch  | Rust（単一バイナリ） |
| 実行者 | 開発者のみ        | エンドユーザー       |
| 頻度   | モデル改善時のみ  | 常時（0.25秒間隔）   |
| 依存   | 大量（PyTorch等） | 最小限（.onnxのみ）  |
| サイズ | 500MB-1GB         | 10-30MB              |

**結論**:

- ✅ **学習**: PoCのままで変更なし
- ✅ **新規追加**: ONNX変換スクリプトのみ
- ✅ **推論**: Rust + ONNX Runtimeで実装
- ✅ **エンドユーザー**: Python不要、単一バイナリ実行

## 将来の展望：学習環境のRust化

### 質問: 学習もRustで完結できるか？

**回答**: 技術的には可能だが、現時点では**推奨しない**。

### Rustでの学習実装の可能性

#### 利用可能なフレームワーク

主に**burn**フレームワーク（v0.15、現在beta版）が選択肢となります:

```rust
// burn フレームワークでの学習実装例
use burn::{
    config::Config,
    module::Module,
    nn::{
        conv::{Conv2d, Conv2dConfig},
        pool::{MaxPool2d, MaxPool2dConfig},
        Linear, LinearConfig, Relu,
    },
    tensor::{backend::Backend, Tensor},
    train::{
        metric::LossMetric,
        ClassificationOutput, TrainOutput, TrainStep, ValidStep,
    },
};

#[derive(Module, Debug)]
pub struct VictoryClassifier<B: Backend> {
    conv1: Conv2d<B>,
    conv2: Conv2d<B>,
    pool: MaxPool2d,
    fc1: Linear<B>,
    fc2: Linear<B>,
}

impl<B: Backend> VictoryClassifier<B> {
    pub fn new(num_classes: usize, device: &B::Device) -> Self {
        Self {
            conv1: Conv2dConfig::new([3, 64], [3, 3])
                .with_padding(burn::nn::PaddingConfig2d::Same)
                .init(device),
            conv2: Conv2dConfig::new([64, 128], [3, 3])
                .with_padding(burn::nn::PaddingConfig2d::Same)
                .init(device),
            pool: MaxPool2dConfig::new([2, 2]).init(),
            fc1: LinearConfig::new(128 * 137 * 248, 256).init(device),
            fc2: LinearConfig::new(256, num_classes).init(device),
        }
    }

    pub fn forward(&self, input: Tensor<B, 4>) -> Tensor<B, 2> {
        let x = self.conv1.forward(input);
        let x = x.relu();
        let x = self.pool.forward(x);

        let x = self.conv2.forward(x);
        let x = x.relu();
        let x = self.pool.forward(x);

        let [batch_size, channels, height, width] = x.dims();
        let x = x.reshape([batch_size, channels * height * width]);

        let x = self.fc1.forward(x);
        let x = x.relu();
        self.fc2.forward(x)
    }
}

// 学習ループの実装
impl<B: Backend> TrainStep<VictoryBatch<B>, ClassificationOutput<B>> for VictoryClassifier<B> {
    fn step(&self, batch: VictoryBatch<B>) -> TrainOutput<ClassificationOutput<B>> {
        let output = self.forward(batch.images);
        let loss = CrossEntropyLoss::new().forward(output.clone(), batch.targets.clone());

        TrainOutput::new(
            self,
            loss.backward(),
            ClassificationOutput::new(loss, output, batch.targets),
        )
    }
}

// データローダーの実装
pub struct VictoryDataset {
    images: Vec<PathBuf>,
    labels: Vec<usize>,
}

impl Dataset<VictoryBatch> for VictoryDataset {
    fn get(&self, index: usize) -> Option<VictoryBatch> {
        // 画像読み込み、前処理、テンソル化
        // OpenCV-rust や image クレート使用
        // ...
    }

    fn len(&self) -> usize {
        self.images.len()
    }
}
```

#### 実装に必要な作業

1. **モデル定義の書き換え** (1-2日)

   - PyTorchのモデルをburnで再実装
   - 層の種類、パラメーター、活性化関数を移植

2. **データローダー実装** (1-2日)

   - 画像読み込み（opencv-rust または image クレート）
   - データ拡張（回転、反転、色調整等）
   - バッチ化、シャッフル

3. **学習ループ実装** (1-2日)

   - TrainStep、ValidStep トレイトの実装
   - 損失関数（CrossEntropyLoss）
   - 最適化器（Adam、SGD）
   - 学習率スケジューリング

4. **メトリクス・ロギング** (0.5-1日)

   - 精度、損失のトラッキング
   - TensorBoard相当の可視化
   - チェックポイント保存

5. **推論への統合** (0.5-1日)
   - 学習済みモデルの保存形式
   - 推論コードでの読み込み

**合計実装コスト**: 4-8日

### エコシステムの成熟度比較

#### PyTorch（Python）

| 項目           | 状態                                           |
| -------------- | ---------------------------------------------- |
| バージョン     | v2.x（安定版、2016年～）                       |
| データ拡張     | torchvision.transforms（豊富）                 |
| 事前学習モデル | torchvision.models（100種類以上）              |
| 可視化         | TensorBoard、Weights & Biases                  |
| デバッグ       | PyCharm、VS Code、Jupyter                      |
| コミュニティ   | 巨大（Stack Overflow、GitHub、論文再現コード） |
| ドキュメント   | 充実（公式、チュートリアル、書籍多数）         |

#### burn（Rust）

| 項目           | 状態                                         |
| -------------- | -------------------------------------------- |
| バージョン     | v0.15（**beta版**、2022年～）                |
| データ拡張     | 自前実装が必要（一部提供）                   |
| 事前学習モデル | 限定的（HuggingFaceからの移植が必要）        |
| 可視化         | 基本的な機能のみ                             |
| デバッグ       | cargo、rust-analyzer（ML特化ツールは少ない） |
| コミュニティ   | 小規模（Discord、GitHub Issuesが主）         |
| ドキュメント   | 基本的なものは整備、応用例は少ない           |

### 開発効率の比較

#### 典型的なCNN学習実装の工数

| タスク                 | PyTorch | burn（Rust） | 比率       |
| ---------------------- | ------- | ------------ | ---------- |
| モデル定義             | 0.5日   | 1-2日        | 2-4x       |
| データローダー         | 0.5日   | 1-2日        | 2-4x       |
| 学習ループ             | 0.5日   | 1-2日        | 2-4x       |
| データ拡張             | 0.5日   | 1-2日        | 2-4x       |
| 可視化・ロギング       | 0.5日   | 1-2日        | 2-4x       |
| デバッグ・トラブル対処 | 0.5日   | 2-4日        | 4-8x       |
| **合計**               | **3日** | **9-14日**   | **3-4.7x** |

**主な差の理由**:

- エコシステムの成熟度差
- サンプルコード・ドキュメントの量
- エラーメッセージの質（Rustのコンパイルエラーは詳細だが、ML特有の問題には弱い）
- コミュニティサポートの差

### 段階的移行戦略（もし将来実施する場合）

#### フェーズ1: 推論のみRust化（今回）

**期間**: 1-2週間

```
学習: Python（PyTorch）
  ↓ ONNX変換
推論: Rust（ONNX Runtime）
```

**メリット**:

- 開発コスト最小
- エンドユーザー環境の簡素化（Python不要）
- 安定性確保

#### フェーズ2: データセット構築のRust化（オプション）

**期間**: 1-2週間（追加）

```
データセット構築: Rust（opencv-rust、image）
学習: Python（PyTorch）
推論: Rust（ONNX Runtime）
```

**メリット**:

- 画像処理の高速化（大量画像処理時に有効）
- 単一言語でデータパイプライン構築
- Python依存をさらに削減

**実装内容**:

- `build_dataset.py` を Rust で再実装
- OpenCV-rustで画像読み込み、クロップ
- ファイル操作、ディレクトリ構造構築

#### フェーズ3: 学習のRust化（長期的検討）

**期間**: 2-4週間（追加）

**前提条件**:

- burn v1.0 リリース（安定版）
- エコシステムの充実（データ拡張、可視化ツール）
- コミュニティサポートの拡大
- **推定時期**: 3-5年後

```
データセット構築: Rust
学習: Rust（burn）
推論: Rust（burn または ONNX Runtime）
```

**メリット**:

- 完全なRust化（Python環境不要）
- 型安全性によるバグ削減
- メモリ安全性の保証
- クロスプラットフォームビルドの簡素化

**デメリット**:

- 開発コスト 3-5倍
- エコシステムの未成熟（現時点）
- コミュニティサポート不足
- デバッグの難易度上昇

### 現時点での推奨事項

#### ❌ 学習のRust化は現時点では推奨しない

**理由**:

1. **エコシステムの未成熟**

   - burn v0.15 はまだbeta版
   - データ拡張ライブラリが限定的
   - 可視化ツールが不足

2. **開発コストが高い**

   - 実装に 3-4.7倍 の時間
   - デバッグに追加時間
   - コミュニティサポートが少ない

3. **学習頻度が低い**

   - モデル改善は月に数回程度
   - クリティカルパスではない
   - Python環境でも十分実用的

4. **Rust化のメリットが限定的**
   - 学習は開発者のみが実行（エンドユーザーには無関係）
   - 実行速度はGPU性能に依存（言語の差は小さい）
   - 型安全性よりも実験速度が重要

#### ✅ 推奨する方針

**短期（今回の実装）**:

- 学習: Python（PyTorch）のまま
- 推論: Rust（ONNX Runtime）で実装
- 分離により両者の利点を活用

**中期（1-2年後）**:

- burn v1.0 のリリース状況を監視
- 必要に応じて **データセット構築のみRust化** を検討
- 学習は引き続きPython

**長期（3-5年後）**:

- burnエコシステムの成熟を待つ
- その時点で**学習のRust化**を再検討
- コストとメリットを再評価

### コスト・メリット分析まとめ

| 項目                     | PyTorch学習 + ONNX推論（推奨） | Rust学習 + Rust推論    |
| ------------------------ | ------------------------------ | ---------------------- |
| 学習実装コスト           | 0日（既存）                    | 9-14日                 |
| 推論実装コスト           | 2-3日（ONNX統合）              | 2-3日                  |
| **合計実装コスト**       | **2-3日**                      | **11-17日**            |
| 開発効率                 | 高（豊富なツール）             | 低（限定的）           |
| エンドユーザー環境       | Rustのみ                       | Rustのみ               |
| 開発者環境               | Python + Rust                  | Rustのみ               |
| 学習時の実験速度         | 高速（試行錯誤が容易）         | 低速（コンパイル待ち） |
| モデル改善時の作業フロー | Python環境で実験→ONNX変換      | Rust環境で全作業       |
| メンテナンスコスト       | 低（コミュニティ大）           | 高（自力解決）         |

**結論**: 現時点では **PyTorch学習 + ONNX推論** が最適。学習のRust化は、burnが成熟してから（3-5年後）再検討すべき。

これにより、**開発の柔軟性**と**配布の簡便さ**を両立できます。
